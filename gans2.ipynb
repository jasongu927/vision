{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gans2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasongu927/vision/blob/master/gans2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE-iuHNDAqWO",
        "colab_type": "code",
        "outputId": "c7e82743-fccb-4825-8e6c-24bf7d6e6d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#can tensorboard work!\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu3de8X0HPQQ",
        "colab_type": "code",
        "outputId": "cb7a9325-4e46-420a-92b2-0606b2be2d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "USE_GPU = True\n",
        "if device_name != '/device:GPU:0':\n",
        "  USE_GPU = False\n",
        "  #raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "#mnist=input_data.read_data_sets(\"MNIST_data\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKn4_5zelJQM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhyRG1L5DMzs",
        "colab_type": "code",
        "outputId": "0969bad5-8a0e-4c1d-c1c0-24c9a265b63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "#from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "tfgan = tf.contrib.gan\n",
        "framework = tf.contrib.framework\n",
        "\n",
        "queues = tf.contrib.slim.queues\n",
        "layers = tf.contrib.layers\n",
        "ds = tf.contrib.distributions\n",
        "leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a9a071e23d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtfgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O37HGAJ_j5ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model_fn(z): \n",
        "    z, conditioning = z\n",
        "    #with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
        "\n",
        "    net = tf.keras.layers.Dense(1024)(z)\n",
        "    #net = tf.keras.layers.BatchNormalization(momentum=0.9)(net)\n",
        "    net = tf.keras.layers.ReLU()(net)\n",
        "    net = tfgan.features.condition_tensor_from_onehot(net, conditioning)\n",
        "    net = tf.keras.layers.Dense(7*7*256)(net)\n",
        "    #net = tf.keras.layers.BatchNormalization()(net)\n",
        "    net = tf.keras.layers.ReLU()(net)\n",
        "\n",
        "    net = tf.keras.layers.Reshape((7, 7, 256))(net)\n",
        "\n",
        "    net = tf.keras.layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding='same', use_bias=True)(net)\n",
        "    #net = tf.keras.layers.BatchNormalization()(net)\n",
        "    #net = tf.keras.layers.LeakyReLU()(net)\n",
        "\n",
        "\n",
        "    net = tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=True)(net)\n",
        "    net = tf.keras.layers.BatchNormalization()(net)\n",
        "    net = tf.keras.layers.LeakyReLU()(net)\n",
        "\n",
        "    net = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(net)\n",
        "\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czLfmMZkktBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_model_fn(x, unused_conditioning):\n",
        "    #with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):\n",
        "\n",
        "    \n",
        "    net = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(x)\n",
        "    #net = tf.keras.layers.BatchNormalization(momentum=0.9)(net)\n",
        "    net = tf.keras.layers.LeakyReLU()(net)\n",
        "\n",
        "    net = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(net)\n",
        "    \n",
        "    net = tf.keras.layers.Flatten()(net)\n",
        "    net = tf.keras.layers.Dense(256)(net)\n",
        "    net = tf.keras.layers.LeakyReLU()(net)\n",
        "    net = tf.keras.layers.BatchNormalization(momentum=0.9)(net)\n",
        "    net = tf.keras.layers.Dropout(0.3)(net)\n",
        "    net = tf.keras.layers.Dense(1)(net)\n",
        "    \"\"\"\n",
        "    input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
        "    net = tf.keras.layers.Conv2D(64, (3, 3), name=\"conv1\", strides=(2,2))(input_layer)\n",
        "    net = tf.keras.layers.Conv2D(64, (3, 3), name=\"conv2\", strides=(2,2))(net)\n",
        "    net = tf.keras.layers.MaxPooling2D((2,2), 1)(net)\n",
        "    net = tf.keras.layers.Flatten()(net)\n",
        "    net = tf.keras.layers.Dense(128*2, activation=tf.nn.relu6)(net)\n",
        "    net = tf.keras.layers.Dropout(0.25)(net)\n",
        "    net = tf.keras.layers.Dense(units=1)(net)\"\"\"\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOXV4qiRDQJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_fn(feat, weight_decay=2.5e-5, is_training=True):\n",
        "    \"\"\"Simple generator to produce MNIST images.\n",
        "    \n",
        "    Args:\n",
        "        noise: A single Tensor representing noise.\n",
        "        weight_decay: The value of the l2 weight decay.\n",
        "        is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n",
        "            norm uses the exponential moving average collected from population \n",
        "            statistics.\n",
        "    \n",
        "    Returns:\n",
        "        A generated image in the range [-1, 1].\n",
        "    \"\"\"\n",
        "    #noise , one_hot_labels = feat\n",
        "    noise = feat[0]\n",
        "    one_hot_labels = feat[1]\n",
        "    with tf.device('/gpu:0'):\n",
        "      with framework.arg_scope(\n",
        "        [layers.fully_connected, layers.conv2d_transpose],\n",
        "        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,\n",
        "        weights_regularizer=layers.l2_regularizer(weight_decay)),\\\n",
        "        framework.arg_scope([layers.batch_norm], is_training=is_training,\n",
        "                        zero_debias_moving_mean=True):\n",
        "        net = layers.fully_connected(noise, 1024)\n",
        "        net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
        "        net = layers.fully_connected(net, 7 * 7 * 256)\n",
        "        net = tf.reshape(net, [-1, 7, 7, 256])\n",
        "        #net = layers.conv2d_transpose(net, 256, [5, 5], stride=1)\n",
        "        net = layers.conv2d_transpose(net, 128, [5, 5], stride=2)\n",
        "        net = layers.conv2d_transpose(net, 64, [5, 5], stride=2)\n",
        "        # Make sure that generator output is in the same range as `inputs`\n",
        "        # ie [-1, 1].\n",
        "        net = layers.conv2d(net, 1, 4, normalizer_fn=None, activation_fn=tf.tanh)\n",
        "\n",
        "        return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGaxF1MrDS4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_fn(img, conditioning, weight_decay=2.5e-5,\n",
        "                     is_training=True):\n",
        "    \"\"\"Discriminator network on MNIST digits.\n",
        "    \n",
        "    Args:\n",
        "        img: Real or generated MNIST digits. Should be in the range [-1, 1].\n",
        "        unused_conditioning: The TFGAN API can help with conditional GANs, which\n",
        "            would require extra `condition` information to both the generator and the\n",
        "            discriminator. Since this example is not conditional, we do not use this\n",
        "            argument.\n",
        "        weight_decay: The L2 weight decay.\n",
        "        is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n",
        "            norm uses the exponential moving average collected from population \n",
        "            statistics.\n",
        "    \n",
        "    Returns:\n",
        "        Logits for the probability that the image is real.\n",
        "    \"\"\"\n",
        "    _, one_hot_labels = conditioning\n",
        "    with tf.device('/gpu:0'):    \n",
        "      with framework.arg_scope(\n",
        "        [layers.conv2d, layers.fully_connected],\n",
        "        activation_fn=leaky_relu, normalizer_fn=None,\n",
        "        weights_regularizer=layers.l2_regularizer(weight_decay),\n",
        "        biases_regularizer=layers.l2_regularizer(weight_decay)):\n",
        "        net = layers.conv2d(img, 64, [5, 5], stride=2)\n",
        "        net = tfgan.features.condition_tensor_from_onehot(net, one_hot_labels)\n",
        "        net = layers.conv2d(net, 128, [5, 5], stride=2)\n",
        "        #net = layers.conv2d(net, 256, [5, 5], stride=1)\n",
        "        net = layers.flatten(net)\n",
        "        with framework.arg_scope([layers.batch_norm], is_training=is_training):\n",
        "            net = layers.fully_connected(net, 1024, normalizer_fn=layers.batch_norm)\n",
        "            net = tf.keras.layers.Dropout(0.25)(net)\n",
        "        return layers.linear(net, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D95GDbAnDVoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras as keras \n",
        "\n",
        "((train_data, train_labels),\n",
        " (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()\n",
        "print(train_data.shape)\n",
        "\n",
        "train_data = train_data/np.float32(255)\n",
        "train_labels = train_labels.astype(np.int32)  # not required\n",
        "\n",
        "eval_data = eval_data/np.float32(255)\n",
        "eval_labels = eval_labels.astype(np.int32)  # not required\n",
        "train_data_1 = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "eval_data_1 = eval_data.reshape(eval_data.shape[0], 28, 28, 1)\n",
        "\n",
        "train_labels_1 = keras.utils.to_categorical(train_labels, 10)\n",
        "eval_labels_1 = keras.utils.to_categorical(eval_labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376xqIn4yDTF",
        "colab_type": "text"
      },
      "source": [
        "Using Estimator API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQnB5z5wyE0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "\n",
        "def _get_train_input_fn(batch_size, noise_dims):\n",
        "    def train_input_fn(): #try with tf.data\n",
        "        real_images, labels = tf.train.batch((train_data_1, train_labels_1), batch_size=batch_size, enqueue_many=True)\n",
        "        noise = tf.random_normal([batch_size, noise_dims])\n",
        "        #return noise, real_images\n",
        "        return ((noise, labels), real_images)\n",
        "    return train_input_fn\n",
        "\n",
        "\n",
        "def _get_predict_input_fn(batch_size, noise_dims):\n",
        "    def predict_input_fn():\n",
        "        noise = tf.random_normal([batch_size, noise_dims])\n",
        "        one_hots = [x%10 for x in range(batch_size)]\n",
        "        return ((noise, tf.one_hot(one_hots,10)),None)\n",
        "        #return noise\n",
        "    return predict_input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryvPmbrFyPk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16#@param\n",
        "NOISE_DIMS = 100#@param\n",
        "NUM_STEPS = 60000/BATCH_SIZE#@param\n",
        "NUM_EPOCHS = 5#@param\n",
        "GEN_LEARNING_RATE = 0.00002 #@param\n",
        "DISC_LEARNING_RATE = 0.00006 #@param\n",
        "\n",
        "print(train_data_1.shape)\n",
        "\n",
        "# Initialize GANEstimator with options and hyperparameters.\n",
        "gan_estimator = tfgan.estimator.GANEstimator(\n",
        "    generator_fn=generator_model_fn,\n",
        "    #discriminator_fn=discriminator_model_fn,\n",
        "    #generator_fn=generator_fn,\n",
        "    discriminator_fn=discriminator_fn,\n",
        "    model_dir = \"tmp/tbr\",\n",
        "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss, \n",
        "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
        "    #generator_loss_fn=tfgan.losses.modified_generator_loss, \n",
        "    #discriminator_loss_fn=tfgan.losses.modified_discriminator_loss,\n",
        "    generator_optimizer=tf.train.AdamOptimizer(GEN_LEARNING_RATE, 0.5),\n",
        "    discriminator_optimizer=tf.train.AdamOptimizer(DISC_LEARNING_RATE, 0.5),\n",
        "    config =tf.estimator.RunConfig(log_step_count_steps=NUM_STEPS/3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbMqrYuW8a0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Train estimator.\n",
        "train_input_fn = _get_train_input_fn(BATCH_SIZE, NOISE_DIMS)\n",
        "start_time = time.time()\n",
        "gan_estimator.train(train_input_fn, steps = 1)\n",
        "time_since_start = (time.time() - start_time) / 60.0\n",
        "print('Time since start: %f m' % time_since_start)\n",
        "print('Steps per min: %f' % (NUM_STEPS / time_since_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpSQcQ7N4hyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "\n",
        "def _get_next(iterable):\n",
        "    try:\n",
        "        return iterable.next()  # Python 2.x.x\n",
        "    except AttributeError:\n",
        "        return iterable.__next__()  # Python 3.x.x\n",
        "\n",
        "\n",
        "# Run inference.\n",
        "predict_input_fn = _get_predict_input_fn(16, NOISE_DIMS)\n",
        "prediction_iterable = gan_estimator.predict(\n",
        "    predict_input_fn, hooks=[tf.train.StopAtStepHook(last_step=1)])\n",
        "predictions = list(prediction_iterable)\n",
        "\n",
        "try: # Close the predict session.\n",
        "    _get_next(prediction_iterable)\n",
        "except StopIteration:\n",
        "    pass\n",
        "\n",
        "# Nicely tile output and visualize.\n",
        "for i in range(1,17):\n",
        "  plt.subplot(4,4,i)\n",
        "  plt.imshow(predictions[i-1].reshape(28,28))\n",
        "plt.show()\n",
        "\n",
        "      \n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  gan_estimator.train(train_input_fn, steps = NUM_STEPS)\n",
        "  time_since_start = (time.time() - start_time) / 60.0\n",
        "  print(\"~\"*40)\n",
        "  print(\"EPOCH %d\" % i)\n",
        "  print(\"~\"*40)\n",
        "  print('Time since start: %f m' % time_since_start)\n",
        "  print('Steps per min: %f' % (NUM_STEPS / time_since_start))\n",
        "\n",
        "  # Run inference.\n",
        "  predict_input_fn = _get_predict_input_fn(16, NOISE_DIMS)\n",
        "  prediction_iterable = gan_estimator.predict(\n",
        "      predict_input_fn)\n",
        "  predictions = [_get_next(prediction_iterable) for _ in xrange(16)]\n",
        "\n",
        "  try: # Close the predict session.\n",
        "      _get_next(prediction_iterable)\n",
        "  except StopIteration:\n",
        "      pass\n",
        "\n",
        "  # Nicely tile output and visualize.\n",
        "  for i in range(1,17):\n",
        "    plt.subplot(4,4,i)\n",
        "    plt.imshow(predictions[i-1].reshape(28,28))\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs7RlzJHBOdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir tmp/tbr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UG0JE4w0DfU",
        "colab_type": "text"
      },
      "source": [
        "# Cats #3\n",
        "from https://github.com/utkd/gans/blob/master/cifar10dcgan.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcajw5H0PCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Reshape\n",
        "from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "%pylab inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaOu_zej0Rfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator(input_layer):\n",
        "  '''\n",
        "  Requires the input layer as input, outputs the model and the final layer\n",
        "  '''\n",
        "  \n",
        "  hid = Dense(128 * 16 * 16, activation='relu')(input_layer)    \n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "  hid = Reshape((16, 16, 128))(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=5, strides=1,padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)    \n",
        "  #hid = Dropout(0.5)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2DTranspose(128, 4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  #hid = Dropout(0.5)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "                      \n",
        "  hid = Conv2D(3, kernel_size=5, strides=1, padding=\"same\")(hid)\n",
        "  out = Activation(\"tanh\")(hid)\n",
        "\n",
        "  model = Model(input_layer, out)\n",
        "  model.summary()\n",
        "  \n",
        "  return model, out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxr16biI0Txg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_discriminator(input_layer):\n",
        "  '''\n",
        "  Requires the input layer as input, outputs the model and the final layer\n",
        "  '''\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=3, strides=1, padding='same')(input_layer)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n",
        "  hid = BatchNormalization(momentum=0.9)(hid)\n",
        "  hid = LeakyReLU(alpha=0.1)(hid)\n",
        "\n",
        "  hid = Flatten()(hid)\n",
        "  hid = Dropout(0.4)(hid)\n",
        "  hid = Dense(128)(hid)\n",
        "  hid = Dropout(0.2)(hid)\n",
        "  out = Dense(1, activation='sigmoid')(hid)\n",
        "\n",
        "  model = Model(input_layer, out)\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model, out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpeh9CCV0Xmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def generate_noise(n_samples, noise_dim):\n",
        "  X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n",
        "  return X\n",
        "\n",
        "def show_imgs(batchidx):\n",
        "  noise = generate_noise(9, 100)\n",
        "  gen_imgs = generator.predict(noise)\n",
        "  plt.figure(figsize=(10,10))\n",
        "  fig, axs = plt.subplots(3, 3)\n",
        "  count = 0\n",
        "  for i in range(3):\n",
        "    for j in range(3):\n",
        "      # Dont scale the images back, let keras handle it\n",
        "      img = image.array_to_img(gen_imgs[count], scale=True)\n",
        "      axs[i,j].imshow(img)\n",
        "      axs[i,j].axis('off')\n",
        "      count += 1\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F93a-D7o0Zvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GAN creation\n",
        "img_input = Input(shape=(32,32,3))\n",
        "discriminator, disc_out = get_discriminator(img_input)\n",
        "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "noise_input = Input(shape=(100,))\n",
        "generator, gen_out = get_generator(noise_input)\n",
        "\n",
        "gan_input = Input(shape=(100,))\n",
        "x = generator(gan_input)\n",
        "gan_out = discriminator(x)\n",
        "gan = Model(gan_input, gan_out)\n",
        "gan.summary()\n",
        "\n",
        "gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TECXnSEC0dEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "# # Get training images\n",
        "(X_train, y_train), (X_test, _) = cifar10.load_data()\n",
        "\n",
        "# Select Cats\n",
        "X_train = X_train[y_train[:,0]==3]\n",
        "print (\"Training shape: {}\".format(X_train.shape))\n",
        "\n",
        "# Normalize data\n",
        "X_train = (X_train - 127.5) / 127.5\n",
        " \n",
        "num_batches = int(X_train.shape[0]/BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwO5t4pG0fB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "N_EPOCHS = 100 #@param\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  cum_d_loss = 0.\n",
        "  cum_g_loss = 0.\n",
        "  \n",
        "  for batch_idx in range(num_batches):\n",
        "    # Get the next set of real images to be used in this iteration\n",
        "    images = X_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n",
        "\n",
        "    noise_data = generate_noise(BATCH_SIZE, 100)\n",
        "    generated_images = generator.predict(noise_data)\n",
        "\n",
        "    # Train on soft labels (add noise to labels as well)\n",
        "    noise_prop = 0.05 # Randomly flip 5% of labels\n",
        "    \n",
        "    # Prepare labels for real data\n",
        "    true_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
        "    flipped_idx = np.random.choice(np.arange(len(true_labels)), size=int(noise_prop*len(true_labels)))\n",
        "    true_labels[flipped_idx] = 1 - true_labels[flipped_idx]\n",
        "    \n",
        "    # Train discriminator on real data\n",
        "    d_loss_true = discriminator.train_on_batch(images, true_labels)\n",
        "\n",
        "    # Prepare labels for generated data\n",
        "    gene_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n",
        "    flipped_idx = np.random.choice(np.arange(len(gene_labels)), size=int(noise_prop*len(gene_labels)))\n",
        "    gene_labels[flipped_idx] = 1 - gene_labels[flipped_idx]\n",
        "    \n",
        "    # Train discriminator on generated data\n",
        "    d_loss_gene = discriminator.train_on_batch(generated_images, gene_labels)\n",
        "\n",
        "    d_loss = 0.5 * np.add(d_loss_true, d_loss_gene)\n",
        "    cum_d_loss += d_loss\n",
        "\n",
        "    # Train generator\n",
        "    noise_data = generate_noise(BATCH_SIZE, 100)\n",
        "    g_loss = gan.train_on_batch(noise_data, np.zeros((BATCH_SIZE, 1)))\n",
        "    cum_g_loss += g_loss\n",
        "\n",
        "  print('  Epoch: {}, Generator Loss: {}, Discriminator Loss: {}'.format(epoch+1, cum_g_loss/num_batches, cum_d_loss/num_batches))\n",
        "  show_imgs(\"epoch\" + str(epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}